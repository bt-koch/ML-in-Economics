---
output:
  bookdown::pdf_document2:
    number_sections: true
    toc: false
indent: true
bibliography: tex/bibliography.bib
biblio-style: text/econometrica.bst
header-includes:
  - \usepackage{floatrow}
  - \floatsetup{capposition=top}
  - \floatplacement{figure}{!ht}
  - \floatplacement{table}{!ht}
  - \usepackage{setspace}
editor_options: 
  markdown: 
    wrap: 72
---

<!-- Setup -------------------------------------------------------------------->
```{r setup, include=FALSE}
rm(list=ls()); gc()
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(randomForest)) install.packages("randomForest")
if(!require(iml)) install.packages("iml")
if(!require(corrplot)) install.packages("corrplot")

library(kableExtra)
library(randomForest)
library(iml)
library(corrplot)

# use computer modern font for plots?
plots_cmfont <- T
# I leave the option to use font "sans" (base R) since using computer modern
# font ("LaTeX-font") required some additional installations on my device:
# R version 4.1.2 (2021-11-01)
# Platform: aarch64-apple-darwin20 (64-bit)
# Running under: macOS Monterey 12.4
# required steps to use computer modern:
# - install Latin Modern Fonts, e.g. from http://www.fontsquirrel.com/fonts/latin-modern-roman
#   make sure to download TTF version
# - import these fonts using extrafont::font_import()
#   here I received following error message:
#    Scanning ttf files in my/path/to/fonts ...   
#    Extracting .afm files from .ttf files...
#    my/path/to/fonts/lmroman10-bold-webfont.ttf : No FontName. Skipping.
#   Downgrading package "Rttf2pt1" from version 1.3.9 to 1.3.8 solved this error
#    package_url <- "https://cran.r-project.org/src/contrib/Archive/Rttf2pt1/Rttf2pt1_1.3.8.tar.gz"
#    install.packages(package_url, repos=NULL, type="source")
# - install package Cairo and use it as the graphics device in knitr::opts_chunk$set
if(plots_cmfont){
  if(!require(extrafont)) install.packages("extrafont")
  library(extrafont)
  loadfonts(device = "all")
  knitr::opts_chunk$set(echo = FALSE, fig.align = "center", dev = "cairo_pdf")
  fontfamily <- "LM Roman 10"
} else {
  knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
  fontfamily <- "sans"
}

load("data/input.RData")
varnames <- list.export[["varnames"]]
data <- list.export[["data"]]
```

<!-- Title Page --------------------------------------------------------------->
```{=tex}
\thispagestyle{plain}
\begin{center}
    \Large
    \textbf{Random forest versus logit models: Which offers better early warning
    of fiscal stress?}
        
    \vspace{0.4cm}
    \large
    replication paper
        
    \vspace{0.4cm}
    \textbf{Bela Tim Koch} \\ 
    
    Machine Learning in Economics (458657) \\
    Spring Semester 2022 \\
    \vspace{0.4cm}
    \textbf{Departement of Economics} \\
    \textbf{University of Bern}
       
    \vspace{0.9cm}
\end{center}
```

<!-- Report ------------------------------------------------------------------->
```{=tex}
\setcounter{page}{1}
\onehalfspace
```

# Introduction and Literature Review

Since the latest Great Recession with its corresponding deterioration of
public finances, the monitoring and prevention of fiscal crises has
become increasingly prominent in the political debate, leading to an
increasing demand for the development of reliable and early indicators
that signal possible fiscal stress. In order to be able to assess
countries' vulnerability to fiscal distress ex-ante, the literature is
increasingly devoted to the development of early warning systems for
fiscal stress, which build on early warning systems for banking and
currency crises [@honda2022]. The standard tool used in the literature
for early warning systems are the signalling approach as well as
discrete dependent variable models, such as logistic regression
[@jarmulska2020].

As an alternative to the traditional methods, early warning models based
on machine learning techniques are proposed, claiming a possible
improvement of prediction accuracy [@beutel2019]. As an example, when
predicting the build-up of banking crises, the early warning system
developed by @casabianca2019 which builds upon a supervised machine
learning algorithm, i.e. Adaptive Boosting, outperforms the traditional
approach of using a logit model. Another example which shows that using
machine learning could drastically improve prediction accuracy is the
early warning system developed by @samitas2020, which reaches an
accuracy of 98.8% when predicting the risk of contagion inside a
financial network using a quadratic support vector machine.

However, a disadvantage many of these machine learning methods compared
to more traditional approaches is the difficulty of understanding how a
result was obtained and are therefore often referred to as a black box
[@ghoddusi2019]. Consequently, the researcher is confronted with a
trade-off between prediction or interpretation. @ghoddusi2019 argue,
that emphasis should be in interpretation for scientific research or
policy decisions, since understanding the relationship and behavior
among different variables is more important than prediction accuracy. In
contrast, more emphasis is to be placed on prediction accuracy in
specific industrial applications.

This paper aims to replicate some of the work done by @jarmulska2020.
Particular emphasis is placed on the comparison of the traditional
method, i.e. a logit model with a least absolute shrinkage and selection
operator, and a model based on machine learning, i.e. an implementation
of the random forest algorithm. As the results are often criticized for
being difficult to interpret, ways of interpreting the developed early
warning model based on random forest are presented.

# Model Describtion

## Performance Metrics

@jarmulska2020 uses sensitivity, specificity, their average as well as
the area under receiver operating curve (AUROC) as measures to assess
the effectiveness of the early warning models. Since sensitivity
corresponds to the proportion of stress episodes correctly classified
whereby specificity corresponds to the proportion of tranquil episodes
correctly classified, these metrics are dependent on the threshold,
which determines whether a period is classified as stress or tranquil
episode [@jarmulska2020]. In this paper, this threshold is specified by
maximizing the weighted sum of sensitivity and specificity. In contrast,
the AUROC is a robust measure, as all possible thresholds are taken into
account in their calculation. This measure represents the area under the
receiver operating curve, which displays the trade-off between the true
positive rate (i.e. sensitivity) and the false positive rate (i.e. 1 -
specificity). Theoretically, the AUROC can be between 0 and 1 (perfect
classifier), whereby random guessing would result in to a value of 0.5
[@fawcett2006].

## Logit Model with LASSO penalisation

@jarmulska2020 implemented two version of discrete dependent variable
models (logit regression), first a standard logit model with ordinary
least squares estimates and second a logit model with a least absolute
shrinkage and selection operator (LASSO) penalization. These models are
often used as the standard econometric approach, which is why they are
used as the benchmark in this study.

Ordinary least squares estimates often have low bias but large variance,
reducing prediction accuracy - the prediction accuracy can sometimes be
improved by shrinking some coefficients towards zero to sacrifice bias
in order to reduce variance of the predicted values and possibly improve
overall prediction accuracy [@tibshirani1996]. To do so, LASSO
penalization as proposed in @tibshirani1996 can be applied. Because of
this characteristics, only the logit LASSO model is considered in this
replication.

Following @hastie2009, the LASSO problem in the Lagrangian form is given
as follows:

```{=tex}
\begin{equation} \label{eq:lasso}
\hat{\beta}^{lasso} = \underset{\beta}{\text{argmin}} \left\{
\frac{1}{2} \sum^N_{i=1}(y_i - \beta_0 - \sum^p_{j=1}x_{ij}\beta_j)^2
+ \lambda \sum^p_{j=1}|\beta_j| \right\}
\end{equation}
```

\noindent whereby $\lambda$ corresponds to the penalization parameter.
As can be seen in Equation \eqref{eq:lasso}, the higher $\lambda$, the
higher the number of coefficients shrunk to zero. Here, $\lambda$ is
chosen by 5-fold cross-validation, maximizing the AUROC.

## Random Forest

As an alternative to the logit model, @jarmulska2020 applied the random
forest algorithm following @breiman2001 as an ensemble of multiple
classification and regression trees for binary classification. A single
tree divides the predictor space into distinct and non-overlapping
regions, whereby an observation is classified depending on the region it
falls into, i.e. at the terminal node. Each non-terminal node
corresponds to a question with a binary response, what determines the
structure of the tree. Since each terminal node assigns the same class
to all observations within this node, minimizing the classification
error at the level of the terminal nodes results in a minimization of
the tree's overall classification error [@jarmulska2020]. To measure the
precision of the fit, the Gini Index, which is used as a loss function
in classification and regression trees, can be utilised
[@jarmulska2020]:

```{=tex}
\begin{equation} \label{eq:gini}
g(w) = \sum_{k \neq j}p_{wk}p_{wj} = \sum_k p_{wk}(1-p_{wk})
\end{equation}
```

\noindent whereby $p_{wj}$ corresponds to the probability distribution
of class $j$ in node $w$.

However, such decision trees might suffer from overfitting, what can be
counteracted by bootstrapping the training set and averaging all the
predictions [@james2013]. If these so called bagged trees are all
influenced by some strong predictors, the single trees might be
correlated resulting again in overfitting. To decorrelate the single
trees, the non-terminal nodes can be forced to consider only a subset of
the predictors, increasing the difference between the single trees,
through which the overfitment problem can be reduced [@james2013]. This
ensemble method corresponds to the random forest algorithm.

*hier noch gewählte Parameter für Random Forest beschreiben*

<!-- \newpage -->

# Data Describtion

## Dependent Variable

The binary dependent variable to be predicted follows the definition in
@dobrescu2011 takes the value of 1 in the case of a fiscal stress event
in the next period and 0 otherwise. According to the definition provided
by @dobrescu2011, an economy faces a fiscal stress event, if at least
one of the following four conditions are fulfilled: (1) the economy
fails to service debt as payments come due as well as if debt exchanges
are distressed (2) the economy receives a large support program by the
International Monetary Fund (3) hyperinflation is prevalent in the
economy (inflation exceeding 35% for advanced economies, 500% for
emerging economies) (4) the economy faces extreme financing constraints,
i.e. the sovereign spread exceeds 1000 basis points or 2 standard
deviations from the country average.

For the analysis, @jarmulska2020 considers
`r length(unique(data$country))` economies
(`r length(unique(data[data$developed == 0,]$country))` developing
economies, `r length(unique(data[data$developed == 1,]$country))`
developed economies) for an observation period for years
`r min(data$year)+2`-`r max(data$year)+2`.
`r round(mean(as.numeric(levels(data$crisis_next_period))[data$crisis_next_period])*100, 1)`%
of the recorded observation are classified as fiscal stress events,
whereby these stress events are not equally distributed across country
groups and over time. Figure \@ref(fig:stress-distr) displays the
distribution of the recorded stress events over time for developing and
developed economies. The data shows that developing economies are more
prone to fiscal stress events with
`r round(mean(as.numeric(levels(data[data$developed == 0,]$crisis_next_period))[data[data$developed == 0,]$crisis_next_period])*100, 1)`%
of all observations classified as fiscal stress events compared to
`r round(mean(as.numeric(levels(data[data$developed == 1,]$crisis_next_period))[data[data$developed == 1,]$crisis_next_period])*100, 1)`%
for developed economies.

```{r stress-distr, fig.height=3, fig.cap="Distribution of Stress Periods"}
data <- list.export[["data"]]
dt <- data


dt$year <- dt$year+2
dt$crisis_next_period <- as.numeric(as.character(dt$crisis_next_period))


dt_developing <- dt[dt$developed == 0,]
dt_developed  <- dt[dt$developed == 1,]

dt_developing <- dt_developing[, names(dt_developing) %in% c("year", "crisis_next_period")]
dt_developing <- aggregate(dt_developing$crisis_next_period,
                           by = list(dt_developing$year),
                           FUN = sum)
colnames(dt_developing) <- c("year", "sum_developing")

dt_developed <- dt_developed[, names(dt_developed) %in% c("year", "crisis_next_period")]
dt_developed <- aggregate(dt_developed$crisis_next_period,
                           by = list(dt_developed$year),
                           FUN = sum)

colnames(dt_developed) <- c("year", "sum_developed")

dt <- merge(x = dt_developing, y = dt_developed,
            by = "year",
            all = T)

par(las = 1, cex = 0.65, family = fontfamily)
barplot(rbind(dt$sum_developing, dt$sum_developed), beside = T,
        names.arg = dt$year, las = 2, col = c("gray30", "gray90"),
        ylab = "Number of fiscal stress events")
legend("topleft",
       legend = c("developing economies", "developed economies"),
       fill = c("gray30", "gray90"))


```

## Explanatory Variables {#expl-vars}

Annual frequency data lagged 2 years in regard to the dependent
variable, hence for an observation period for years
`r min(data$year)`-`r max(data$year)`, are used to train and test the
early warning models. @jarmulska2020 chose a lag of two years to
simulate the reality of how the early warning models could be used in
practice, as the data becomes available with a delay of up to one and a
half years, leaving the decision-makers half a year time to react on the
results of the models.

All variables used for building the early warning model are listed in
Table \@ref(tab:means-table) including their means with the distinction
whether the economy is in a fiscal stress period or not. In addition,
using a Wilcoxon test with statistical significance at 0.05%, Table
\@ref(tab:means-table) shows if the means in tranquil periods and in
stress periods are statistically different and therefore indicating that
the observed variables behave differently in stress periods, hence
giving the variables potentially explanatory power.

\renewcommand{\arraystretch}{0.75}

```{r means-table}
# load relevant data
means.table <- list.export[["means.table"]]

# round numeric columns
num.cols <- sapply(means.table, mode) == "numeric"
means.table[num.cols] <- round(means.table[num.cols], 2)

# define categories
macro_globecon <- c("interest_rate_US", "dyn_GDP_US", "dyn_gdp_china", "oil_yoy",
                    "VIX", "dyn_gdp", "GDP_per_cap")
comp_domdem <- c("overvaluation", "ca_balance", "dyn_export_share", "dyn_fix_cap_form",
                 "cpi", "dyn_consum")
fin <- c("dyn_fx_rate", "diff_priv_credit_gdp")
fisc <- c("net_lending", "public_debt", "interest_on_debt")
labor <- c("diff_unempl", "dyn_prod_dol")

# add categories
means.table$category <- NA
means.table$category <- ifelse(means.table$variable %in% macro_globecon,
                               "Macroeconomic and global economy",
                               means.table$category)
means.table$category <- ifelse(means.table$variable %in% comp_domdem,
                               "Competitiveness and domestic demand",
                               means.table$category)
means.table$category <- ifelse(means.table$variable %in% fin,
                               "Financial",
                               means.table$category)
means.table$category <- ifelse(means.table$variable %in% fisc,
                               "Fiscal",
                               means.table$category)
means.table$category <- ifelse(means.table$variable %in% labor,
                               "Labor market",
                               means.table$category)

# add variable names
means.table <- merge(x = varnames, y = means.table,
                     by = "variable",
                     all = T)
means.table$variable <- NULL

# recode significance
means.table$significant <- ifelse(means.table$significant == T, "yes", "no")

# rename columns
names(means.table)[names(means.table) == "name"] <- "Variable"
names(means.table)[names(means.table) == "all_periods"] <- "All periods"
names(means.table)[names(means.table) == "tranq_periods"] <- "Tranquil periods"
names(means.table)[names(means.table) == "stress_periods"] <- "Stress periods"
names(means.table)[names(means.table) == "p_value"] <- "P-value"
names(means.table)[names(means.table) == "significant"] <- "Significance"

# make table
kbl(means.table[, -which(names(means.table) == "category")], booktabs = T,
    caption = "Means of Explanatory Variables", align = c("l", rep("r", 5))) %>%
  pack_rows(index = table(means.table$category)) %>%
  kable_styling(latex_options="scale_down", font_size = 8)
```

\noindent Pairwise correlation is also examined and visualized in Figure
\@ref(fig:corrplot). Accordingly, the pairwise correlations are
relatively low in most cases, which is why it can be presumed that the
variables contain different information and could thus be relevant for
the early warning model. However, some variables are highly correlated,
which is problematic for econometric models such as the logit model and
therefore should be excluded in the model specification, while high
pairwise correlations is unproblematic for the random forest model and
the variables concerned can be kept in the model specification
[@jarmulska2020].

```{r corrplot, fig.height=4, fig.cap="Pairwise Correlation"}
corr.matrix <- list.export$corr.matrix
par(family = fontfamily)

corrplot(corr.matrix, method = "square", order = "FPC", type = "lower", diag = FALSE,
         tl.cex = 0.6, tl.col = "black", tl.srt = 45, cl.cex=0.7)
```

<!-- \newpage -->

# Empirical Results

## Performance

For each year in the interval from 2006 to 2016, both the logit model
and the random forest model are implemented recursively, trying to
classify the state of the economy two years later. For the logit model,
all variables with pairwise correlations exceeding 65% are excluded to
avoid multicollinearity and biases. The models are fitted twice using
either a binary variable as a explanatory variable to indicate whether
an economy is developed or developing or GDP per capita as a continuous
measure for the state of development of an economy. If the binary
variable is used, interaction terms are added to the logit model, while
the random forest model takes interactions into account by construction
thus making further adjustments redundant [@jarmulska2020].

The performance of the models measured using sensitivity (\% of correctly
classified stress episodes) and specificity (\% of correctly classified tranquil
episodes) depends on the threshold chosen, whereby when the predicted dependent
variable exceeds this threshold, a period is classified as a stress episode. 
Following @jarmulska2020, this threshold is determined by maximizing the weighted
average of sensitivity and specificity, weighting sensitivity by factor 1, 1.5
and 2 relatively to sensitivity. Table \@ref(tab:avg-pred-accur) compares the
performance of the various models summarized as the average performance of the
recursively trained models by year on the corresponding test data, using a
threshold determined by the maximized weighted average using factor 1.5 (note
that the AUROC is not dependent on the threshold chosen [@jarmulska2020]). Thereby
it can be seen that the model using random forest outperforms the logit LASSO 
approach in both specifications and in all performance metrics.

```{r avg-pred-accur}
# get relevant data
dt <- list.export[["results.avg"]]

# manipulate formation
dt[!colnames(dt) %in% c("model", "auc")] <- apply(dt[!colnames(dt) %in% c("model", "auc")], 2, function(x) x*100)
dt[colnames(dt) != "model"] <- apply(dt[colnames(dt) != "model"], 2, function(x) round(x, 2))
```

```{=tex}
\renewcommand{\arraystretch}{1}
\begin{table}[H]
\centering
\begin{tabular}{@{\extracolsep{4pt}}lrrrr@{}}
                                             & \multicolumn{2}{c}{Logit LASSO}                                         & \multicolumn{2}{c}{Random Forest}                                       \\ \cline{2-3} \cline{4-5}
                                             & \multicolumn{1}{c}{\begin{tabular}[c]{@{}r@{}}advanced\\ dummy\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}r@{}}GDP\\ per capita\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}r@{}}advanced\\ dummy\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}r@{}}GDP\\ per capita\end{tabular}} \\ \hline
\begin{tabular}[c]{@{}l@{}}sensitivity (\% of correctly\\ classified stress episodes)\end{tabular}   & `r dt[dt$model == "logit.lasso.DUMMY",]$prop.pos` & `r dt[dt$model == "logit.lasso.GDP",]$prop.pos` & `r dt[dt$model == "rf.DUMMY",]$prop.pos` & `r dt[dt$model == "rf.GDP",]$prop.pos` \\ \hline
\begin{tabular}[c]{@{}l@{}}specificity (\% of correctly\\ classified tranquil episodes)\end{tabular} & `r dt[dt$model == "logit.lasso.DUMMY",]$prop.neg` & `r dt[dt$model == "logit.lasso.GDP",]$prop.neg` & `r dt[dt$model == "rf.DUMMY",]$prop.neg` & `r dt[dt$model == "rf.GDP",]$prop.neg` \\ \hline
\begin{tabular}[c]{@{}l@{}}Average of sensitivity\\ and specificity \end{tabular}                                      & `r dt[dt$model == "logit.lasso.DUMMY",]$avg`      & `r dt[dt$model == "logit.lasso.GDP",]$avg`      & `r dt[dt$model == "rf.DUMMY",]$avg`      & `r dt[dt$model == "rf.GDP",]$avg`      \\ \hline
\begin{tabular}[c]{@{}l@{}}AUROC\\ \phantom{AUROC} \end{tabular}                                        & `r dt[dt$model == "logit.lasso.DUMMY",]$auc`      & `r dt[dt$model == "logit.lasso.GDP",]$auc`      & `r dt[dt$model == "rf.DUMMY",]$auc`      & `r dt[dt$model == "rf.GDP",]$auc`      \\ \hline
\end{tabular}
\caption{\label{tab:avg-pred-accur}Average prediction accuracy of early warning models for years 2009-2018}
\end{table}
```

## Interpretability of Random Forest Algorithm

### Interpretation of logit LASSO vs Random Forest

As seen in \@ref(tab:avg-pred-accur), the model using random forest outperforms
the more traditionally used logit model. Depending on the application, however,
performance alone is not the only relevant factor when choosing a model. Another
central criterion in many use cases is the interpretability of the model, whereby
the logit model benefits from the possibility of applying classical econometric
methods (marginal effects, Wald test). However, there are also various methods
that can be used to interpret the random forest model. These methods are
discussed in the following chapters.

### Variable Importance [@breiman2001] and Shapley Values [@shapley1953]

According to @jarmulska2020 following @breiman2001, variable importance corresponds
to the average improvement of a model's performance caused by the addition of the
respective variable. This improvement can be quantified either as the mean increase
in performance or the mean decrease in impurity measured by the Gini index (see
Equation \@ref(eq:gini)). When correlation is present between the different
variables considered, it is important to be aware that the variable importance
might be biased due to unrealistic observations caused by permutation when
calculating the variable importance. In addition, the magnitude of the results
will be decreased when correlation is present, since the actual importance is
distributed between the correlated variables.


*definition shapley*

Figure \@ref(fig:rf-VI-SV) shows blablas

```{r rf-VI-SV, fig.cap="Variable Importance and Shapley Values of Predictors used", fig.height=3}
# prepare data for plot for variable importance
rf.fit.eval <- list.export[["rf.fit.eval"]]

varimp <- as.data.frame(importance(rf.fit.eval))
varimp$variable <- rownames(varimp)
varimp <- varimp[order(varimp$MeanDecreaseGini, decreasing = T), c(1,2)]
varimp$yaxis <- rev(1:nrow(varimp))

varimp <- merge(x = varimp, y = varnames,
                by = "variable",
                all = T)

varimp <- varimp[order(varimp$MeanDecreaseGini, decreasing = T), c(1:4)]
labs.varimp <- rev(varimp$name)

# prepare data for plot for shapley values
shapley.values <- list.export[["shapley.values"]]

shapley.values <- shapley.values[order(shapley.values$x, decreasing = T), c(1,2)]
shapley.values$yaxis <- rev(1:nrow(shapley.values))

shapley.values <- merge(x = shapley.values, y = varnames,
                        by.x = "Group.1", by.y = "variable",
                        all = T)

shapley.values <- shapley.values[order(shapley.values$x, decreasing = T), c(1:4)]
labs.shapley <- rev(shapley.values$name)

# draw plot
par(mar = c(5.1, 10.5, 2.1, 0.5), mfrow = c(1,2), las=1, cex = 0.75,
    cex.axis = 0.75, mgp = c(1.75, 0.75, 0), family = fontfamily)

# plot 1: variable importance
plot(x = varimp$MeanDecreaseGini, y = varimp$yaxis,
     main = "", xlab = "Breiman's Variable Importance", ylab = "", yaxt = "none",
     pch = 16)
abline(h = 1:20, lty = 3)
axis(side = 2, at = 1:20, labels = labs.varimp)

# plot 2: shapley values
plot(x = shapley.values$x, y = shapley.values$yaxis,
     main = "", xlab = "Shapley Values", ylab = "", yaxt = "none", pch = 16)
abline(h = 1:20, lty = 3)
axis(side = 2, at = 1:20, labels = labs.shapley)
```

### Partial dependence plots [@friedman2001] and Accumulated local effects plots [@apley2020]

*definition schreiben und kommentieren/beschreiben was es konkret bedeutet*
*resp. wie es zu lesen ist*

```{r rf-pdp-alep, fig.cap="Partial Dependence (LHS) and Accumulated Local Effects (RHS) Plots"}

# load data for partial dependence plots
partial.ca_balance  <- list.export[["partial.ca_balance"]]
partial.net_lending <- list.export[["partial.net_lending"]]

# load data for accumulated local effects plots
ale.ca_balance  <- list.export[["ale.ca_balance"]]
ale.net_lending <- list.export[["ale.net_lending"]]

# prepare plot
# x.train.eval.df <- list.export[["x.train.eval.df"]]
x.train.eval.df <- as.data.frame(list.export[["x.train.eval"]])

par(mfrow = c(2,2), mar = c(5.1, 4.1, 1, 2.1), cex = 0.7, cex.axis = 0.75,
    las = 1, mgp = c(2.25, 0.55, 0), family = fontfamily)

# plot upper left: pdp for ca balance
plot(x = partial.ca_balance$ca_balance, y = partial.ca_balance$yhat,
           type = "l", xlab = "Current account balance",
           ylab = "Predicted probability of stress")
rug(x.train.eval.df$ca_balance, ticksize = 0.015)

# plot upper right: ace plot for ca balance
plot(x = ale.ca_balance$results$ca_balance, y = ale.ca_balance$results$.value,
           type = "l", xlab = "Current account balance",
           ylab = "Predicted probability of stress")
rug(x.train.eval.df$ca_balance, ticksize = 0.015)

# plot lower left: pdp for net lending
plot(x = partial.net_lending$net_lending, y = partial.net_lending$yhat,
           type = "l", xlab = "Net lending",
           ylab = "Predicted probability of stress")
rug(x.train.eval.df$net_lending, ticksize = 0.015)

# plot lower right: ace plot net lending 
plot(x = ale.net_lending$results$net_lending, y = ale.net_lending$results$.value,
           type = "l", xlab = "Net lending",
           ylab = "Predicted probability of stress")
rug(x.train.eval.df$net_lending, ticksize = 0.015)
```

<!-- \newpage -->

# Conclusion

*reputation and credibility (p19)?*

*see slide 22 ML VL 10 -> RF outperformed by other methods?*

<!-- References --------------------------------------------------------------->

\newpage

# References

The code and data used for this project can be found in the
corresponding GitHub-repository:
\mbox{\texttt{\url{https://github.com/bt-koch/ML-in-Economics}}}.

\vspace{1cm}

\bibliography{tex/bibliography.bib}
